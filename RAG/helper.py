from llm import chat
from prompts import prompt
from indexing import embed, get_vector_store

inputs = {'origin_port' : 'Singapore', 
          'destination_port' : 'Shanghai', 
          'ship_type' : 'panamax', 
          'dwt' : 75000, 
          'length' : 225, 
          'width' : 32.2,
          'draft' : 12.0,
          'cargo' : 'coal',
          'fuel_type' : 'VLSFO',
          'consumption_model' : '35 mt/day at 14 knots',
          'timeframe' : '20-30 Oct 2025 laycan' }

def input_filter(inputs: dict) -> dict:
    '''This function takes as input the dictionary of inputs given using the dashboard interface
    It filters input values that are useful for RAG pipeline retrieval
    returns a new dictionary containing the required input'''
    relevant_keys = [
        'origin_port',
        'destination_port',
        'ship_type',
        'dwt',
        'cargo',
        'fuel_type',
        'timeframe'
    ]
    
    new = {key: inputs[key] for key in relevant_keys if key in inputs}
    return new

def retrieve(inputs:dict):
    '''This function takes as input the requirements of a charterer
    It formats the inputs into a query to facilitate retrieval 
    Retrieval only selects documents with distance <= 0.8 for relevance
    adds a new key to the inputs dictionary, with the context as value
    returns the input dictionary'''
    
    query = (
        f"As a charterer, I need insights for a {inputs.get('ship_type', 'ship')} "
        f"of about {inputs.get('dwt', 'N/A')} DWT, carrying {inputs.get('cargo', 'unspecified cargo')} "
        f"from {inputs.get('origin_port', 'unknown origin')} to {inputs.get('destination_port', 'unknown destination')} "
        f"during {inputs.get('timeframe', 'an unspecified timeframe')}. "
        f"The ship uses {inputs.get('fuel_type', 'marine fuel')}. "
        f"Please provide information about relevant maritime laws, regulations, fuel economics, "
        f"geopolitical risks, and commodity market reports related to this trade."
    )


    vectorstore = get_vector_store()
    try:
        context = vectorstore.similarity_search_with_score(query, k = 10)
        filtered = [(doc, score) for doc, score in context if score <= 0.8]
        final = [doc.page_content for doc, score in filtered]
    except Exception as e:
        print(f"Error: Retriever failed to retrieve documents. \nReason: {e}")
        return None
    inputs['context'] = final
    return inputs
        
def generate(inputs:dict):
    '''This function takes as input the inputs dictionary that is returned from the retrieve function 
    ensure that the input function contains the context dictionary 
    It will format the prompt as using the inputs and invoke the llm 
    it returns a string generated by the LLM'''

    necessary_keys = ['cargo',
                      'destination_port',
                      'origin_port', 
                      'cargo',
                      'fuel_type',
                      'context']
    for key in necessary_keys:
        if key not in inputs.keys():
            raise Exception(f"Missing {key} from inputs! Check inputs again!")
    response = chat.invoke(inputs)
    return response

def sample_main(inputs):
    inputs = input_filter(inputs)
    inputs = retrieve(inputs)
    answer = generate(inputs)
    print(answer)


sample_main(inputs)